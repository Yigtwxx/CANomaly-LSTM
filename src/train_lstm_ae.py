# train_lstm_ae.py
# ==============================================================
# ðŸ“Œ DOSYA AMACI: LSTM Autoencoder ile CAN-Bus Anomali Tespiti
# - can_data.csv'den Ã¶zellik Ã§Ä±karma (payload, iat, id-onehot)
# - Sliding window ile sekans oluÅŸturma
# - SADECE normal pencerelerle eÄŸitim (unsupervised/semi-supervised)
# - Yeniden Ã¼retim hatasÄ± hesaplama ve eÅŸik belirleme
# - Classification report ve hata kaydÄ±
# ==============================================================

# Gerekli kÃ¼tÃ¼phaneler
import numpy as np
import pandas as pd
import torch                               # PyTorch ana modÃ¼l
from torch import nn                       # Sinir aÄŸÄ± katmanlarÄ±
from torch.utils.data import Dataset, DataLoader  # Veri yÃ¶netimi
from sklearn.preprocessing import StandardScaler, OneHotEncoder  # Normalizasyon
from sklearn.model_selection import train_test_split  # Veri bÃ¶lme
from sklearn.metrics import precision_recall_fscore_support, classification_report

# -------------------- HÄ°PERPARAMETRELER --------------------
DATA_PATH = "can_data.csv"   # Girdi CSV dosyasÄ±
WINDOW_SIZE = 50             # Pencere uzunluÄŸu (kaÃ§ mesaj bir sekans oluÅŸturur)
STRIDE = 5                   # KaydÄ±rma adÄ±mÄ± (1=yoÄŸun, 5=hÄ±zlÄ± iÅŸlem)
BATCH_SIZE = 64              # Mini-batch boyutu
EPOCHS = 40                  # EÄŸitim epoch sayÄ±sÄ±
LR = 1e-3                    # Ã–ÄŸrenme oranÄ± (learning rate)
HIDDEN_SIZE = 64             # LSTM gizli katman boyutu
LATENT_SIZE = 32             # SÄ±kÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ temsil boyutu (bottleneck)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # GPU varsa kullan
OUT_ERRORS = "recon_errors.csv"  # Hata kayÄ±t dosyasÄ±
print("Device:", DEVICE)

# -------------------- 1) VERÄ°YÄ° YÃœKLE --------------------
df = pd.read_csv(DATA_PATH)

# b0-b7 sÃ¼tunlarÄ± yoksa 0 ile doldur (gÃ¼venlik kontrolÃ¼)
for k in range(8):
    if f"b{k}" not in df.columns:
        df[f"b{k}"] = 0

# Zaman sÄ±rasÄ±na gÃ¶re sÄ±rala
df = df.sort_values("timestamp").reset_index(drop=True)

# IAT (Inter-Arrival Time): ArdÄ±ÅŸÄ±k mesajlar arasÄ± zaman farkÄ±
# Anomali tespitinde Ã¶nemli bir Ã¶zellik (burst saldÄ±rÄ±larÄ±nÄ± yakalar)
df["iat"] = df["timestamp"].diff().fillna(0.0)

# CAN ID'leri sayÄ±sallaÅŸtÄ±r (indeksleme)
unique_ids = sorted(df["can_id"].unique())
id_map = {v:i for i,v in enumerate(unique_ids)}  # ID -> indeks eÅŸlemesi
df["id_idx"] = df["can_id"].map(id_map)

# One-Hot Encoding: Kategorik ID'leri binary vektÃ¶rlere Ã§evir
# Ã–rn: 3 ID varsa -> [1,0,0], [0,1,0], [0,0,1]
onehot = OneHotEncoder(sparse=False, handle_unknown="ignore")
id_onehot = onehot.fit_transform(df[["id_idx"]])  # (N, n_ids)

# Payload ve IAT Ã¶zelliklerini numpy array'e Ã§evir
payload_cols = [f"b{k}" for k in range(8)]
X_payload = df[payload_cols].values.astype(np.float32)   # (N, 8)
X_iat = df[["iat"]].values.astype(np.float32)            # (N, 1)

# Ã–zellik vektÃ¶rÃ¼nÃ¼ birleÅŸtir: [ID-onehot | payload(8) | iat(1)]
X = np.concatenate([id_onehot.astype(np.float32), X_payload, X_iat], axis=1)
print("Feature shape:", X.shape)

# SÃ¼rekli Ã¶zellikleri normalize et (one-hot hariÃ§)
# StandardScaler: ortalama=0, std=1 yapar (model eÄŸitimini kolaylaÅŸtÄ±rÄ±r)
num_id_cols = id_onehot.shape[1]
scaler = StandardScaler()
X[:, num_id_cols:] = scaler.fit_transform(X[:, num_id_cols:])

# Etiketleri al (varsa), yoksa hepsini normal varsay
labels = df.get("label", pd.Series(0, index=df.index)).values

# -------------------- 2) SLIDING WINDOW OLUÅžTUR --------------------
def make_windows(arr, window_size=WINDOW_SIZE, stride=STRIDE):
    """
    Zaman serisi verisini Ã¼st Ã¼ste binen pencerelere bÃ¶l.
    arr: (N, F) ÅŸeklinde girdi -> (num_windows, window_size, F) Ã§Ä±ktÄ±
    Sliding window: LSTM gibi sekans modelleri iÃ§in gerekli format
    """
    windows = []
    for start in range(0, arr.shape[0] - window_size + 1, stride):
        windows.append(arr[start:start+window_size])
    return np.stack(windows)

windows = make_windows(X, WINDOW_SIZE, STRIDE)  # (W, T, F)

# Her pencere iÃ§in etiket belirle
# Pencere iÃ§inde EN AZ 1 anomali varsa -> pencere anomali (label=1)
win_labels = []
for start in range(0, len(df) - WINDOW_SIZE + 1, STRIDE):
    seg = labels[start:start+WINDOW_SIZE]
    win_labels.append(int(seg.sum() > 0))  # Herhangi bir 1 varsa True
win_labels = np.array(win_labels)

print("windows shape:", windows.shape, "num anomalies:", win_labels.sum())

# -------------------- 3) SADECE NORMAL PENCERELERLE EÄžÄ°T --------------------
# Anomali tespiti prensibi: Modeli sadece normal veriyle eÄŸit
# Model normal davranÄ±ÅŸÄ± Ã¶ÄŸrenir, anomalileri yeniden Ã¼retemez -> yÃ¼ksek hata

normal_idx = np.where(win_labels==0)[0]  # Normal pencerelerin indeksleri
train_idx, val_idx = train_test_split(normal_idx, test_size=0.2, random_state=42)
X_train = windows[train_idx]
X_val = windows[val_idx]

# Test: TÃœM pencereler (normal + anomali) - deÄŸerlendirme iÃ§in
X_test = windows
y_test = win_labels

# PyTorch Dataset sÄ±nÄ±fÄ± (veri yÃ¼kleyici iÃ§in)
class WinDataset(Dataset):
    def __init__(self, arr):
        self.x = torch.tensor(arr, dtype=torch.float32)
    def __len__(self):
        return self.x.size(0)
    def __getitem__(self, idx):
        return self.x[idx]

# DataLoader: Mini-batch halinde veri saÄŸlar
train_loader = DataLoader(WinDataset(X_train), batch_size=BATCH_SIZE, shuffle=True)
val_loader   = DataLoader(WinDataset(X_val),   batch_size=BATCH_SIZE, shuffle=False)
test_loader  = DataLoader(WinDataset(X_test),  batch_size=BATCH_SIZE, shuffle=False)

# -------------------- 4) MODEL: LSTM AUTOENCODER --------------------
class LSTMAE(nn.Module):
    """
    LSTM Autoencoder mimarisi:
    
    ENCODER: LSTM -> son zaman adÄ±mÄ± -> FC -> latent (sÄ±kÄ±ÅŸtÄ±rÄ±lmÄ±ÅŸ temsil)
    DECODER: latent -> FC -> LSTM -> yeniden oluÅŸturulmuÅŸ sekans
    
    Ã‡alÄ±ÅŸma Prensibi:
    - Normal veriyle eÄŸitilir, normal paternleri Ã¶ÄŸrenir
    - Anomali gelince yeniden Ã¼retemez -> yÃ¼ksek reconstruction error
    - Hata > eÅŸik ise anomali olarak iÅŸaretle
    """
    def __init__(self, input_size, hidden_size=HIDDEN_SIZE, latent_size=LATENT_SIZE):
        super().__init__()
        # Encoder: Girdi sekansÄ±nÄ± gizli duruma Ã§evir
        self.enc = nn.LSTM(input_size, hidden_size, batch_first=True)
        # Bottleneck: Gizli durumu latent boyuta sÄ±kÄ±ÅŸtÄ±r
        self.fc1 = nn.Linear(hidden_size, latent_size)
        # Decoder hazÄ±rlÄ±k: Latent'i gizli boyuta geniÅŸlet
        self.fc2 = nn.Linear(latent_size, hidden_size)
        # Decoder: Gizli durumdan sekansÄ± yeniden oluÅŸtur
        self.dec = nn.LSTM(hidden_size, input_size, batch_first=True)

    def forward(self, x):
        # Encoder: (B, T, F) -> (B, T, H) -> son adÄ±m (B, H)
        enc_out, _ = self.enc(x)
        last = enc_out[:, -1, :]           # Son zaman adÄ±mÄ±nÄ±n Ã§Ä±ktÄ±sÄ±
        
        # Bottleneck: (B, H) -> (B, Z)
        z = torch.tanh(self.fc1(last))     # tanh: -1 ile 1 arasÄ± normalize
        
        # Decoder hazÄ±rlÄ±k: (B, Z) -> (B, H) -> (B, T, H)
        dec_in = torch.relu(self.fc2(z)).unsqueeze(1).repeat(1, x.size(1), 1)
        
        # Decoder: (B, T, H) -> (B, T, F)'e Ã§evir
        dec_out, _ = self.dec(dec_in)
        return dec_out  # Yeniden oluÅŸturulmuÅŸ sekans

# Model, optimizer ve loss fonksiyonu oluÅŸtur
model = LSTMAE(input_size=windows.shape[2]).to(DEVICE)
opt = torch.optim.Adam(model.parameters(), lr=LR)  # Adam optimizer
crit = nn.MSELoss()  # Mean Squared Error - reconstruction loss

# -------------------- 5) EÄžÄ°TÄ°M DÃ–NGÃœSÃœ --------------------
for epoch in range(1, EPOCHS+1):
    # ---- Train (EÄŸitim) ----
    model.train()  # Dropout, BatchNorm gibi katmanlarÄ± eÄŸitim moduna al
    tloss = 0.0
    for b in train_loader:
        b = b.to(DEVICE)
        opt.zero_grad()          # GradyanlarÄ± sÄ±fÄ±rla
        rec = model(b)           # Forward pass - yeniden Ã¼retim
        loss = crit(rec, b)      # KayÄ±p hesapla (girdi vs Ã§Ä±ktÄ± farkÄ±)
        loss.backward()          # Backpropagation
        opt.step()               # AÄŸÄ±rlÄ±klarÄ± gÃ¼ncelle
        tloss += loss.item()*b.size(0)
    tloss /= len(train_loader.dataset)

    # ---- Validation (DoÄŸrulama) ----
    model.eval()  # DeÄŸerlendirme moduna al
    vloss = 0.0
    with torch.no_grad():  # Gradyan hesaplama kapalÄ± (hÄ±z iÃ§in)
        for b in val_loader:
            b = b.to(DEVICE)
            rec = model(b)
            loss = crit(rec, b)
            vloss += loss.item()*b.size(0)
    vloss /= len(val_loader.dataset)

    print(f"Epoch {epoch}/{EPOCHS} train_loss={tloss:.6f} val_loss={vloss:.6f}")

# -------------------- 6) TÃœM PENCERELERDE HATA HESABI --------------------
def compute_errors(dataloader):
    """
    Her pencere iÃ§in ortalama MSE (reconstruction error) hesapla.
    YÃ¼ksek hata = model bu pencereyi iyi yeniden Ã¼retemedi = anomali ÅŸÃ¼phesi
    """
    model.eval()
    errs = []
    with torch.no_grad():
        for b in dataloader:
            b = b.to(DEVICE)
            rec = model(b)
            # Her pencere iÃ§in MSE: (B, T, F) boyutlarÄ±nda ortalama
            batch_err = torch.mean((rec - b)**2, dim=(1,2)).cpu().numpy()
            errs.append(batch_err)
    return np.concatenate(errs)

all_errors = compute_errors(test_loader)

# -------------------- 7) EÅžÄ°K SEÃ‡Ä°MÄ° (F1 MAKSÄ°MÄ°ZE) --------------------
# En iyi threshold'u F1 skorunu maksimize ederek bul
best_thr, best_f1 = None, -1
for thr in np.linspace(all_errors.min(), all_errors.max(), 200):
    preds = (all_errors > thr).astype(int)  # Hata > eÅŸik ise anomali
    p, r, f, _ = precision_recall_fscore_support(y_test, preds, average="binary", zero_division=0)
    if f > best_f1:
        best_f1 = f
        best_thr = thr
print("best thr:", best_thr, "best F1:", best_f1)

# SeÃ§ilen eÅŸikle final tahminler ve rapor
preds = (all_errors > best_thr).astype(int)
print(classification_report(y_test, preds, zero_division=0))

# -------------------- 8) HATA + ETÄ°KET CSV KAYDI --------------------
# GÃ¶rselleÅŸtirme ve analiz iÃ§in hatalarÄ± kaydet
out_df = pd.DataFrame({"error": all_errors, "label": y_test.astype(int)})
out_df.to_csv(OUT_ERRORS, index=False)
print("Saved errors to", OUT_ERRORS)
